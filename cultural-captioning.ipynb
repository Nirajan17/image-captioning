{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3081988,"sourceType":"datasetVersion","datasetId":1864860},{"sourceId":7142991,"sourceType":"datasetVersion","datasetId":3829912}],"dockerImageVersionId":30176,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport csv \nfrom glob import glob\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-08T04:00:50.185703Z","iopub.execute_input":"2023-12-08T04:00:50.185926Z","iopub.status.idle":"2023-12-08T04:00:50.191205Z","shell.execute_reply.started":"2023-12-08T04:00:50.185899Z","shell.execute_reply":"2023-12-08T04:00:50.190419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate, CuDNNLSTM\nfrom keras.models import Sequential, Model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T04:01:06.834716Z","iopub.execute_input":"2023-12-08T04:01:06.835440Z","iopub.status.idle":"2023-12-08T04:01:06.841883Z","shell.execute_reply.started":"2023-12-08T04:01:06.835402Z","shell.execute_reply":"2023-12-08T04:01:06.841101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Images","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# #with open(\"../input/pertrained/images_features.pkl\",\"rb\") as f:\n# #    pickle.dump(images_features , f) \n\nimage_features = {}\nwith open(\"/kaggle/input/modal-and-pickle/combined_resnet50_images_features.pkl\",\"rb\") as f:\n    images_features=pickle.load(f)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-08T04:01:16.263483Z","iopub.execute_input":"2023-12-08T04:01:16.264074Z","iopub.status.idle":"2023-12-08T04:01:18.801265Z","shell.execute_reply.started":"2023-12-08T04:01:16.264020Z","shell.execute_reply":"2023-12-08T04:01:18.800604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Captions and Creating Vocabulary","metadata":{}},{"cell_type":"code","source":"caption_path = '/kaggle/input/stanford-paragraph-dataset-in-nepali/stanford_cultural_nepalis.csv'\n\nwith open(caption_path) as csvfile:\n    captions=csv.reader(csvfile, delimiter=',', quotechar='\"')    \n    captions_dict_train,captions_dict_test,captions_dict_val= {},{},{}\n    for i in captions:\n        try:\n            img_name = i[0]  + \".jpg\"\n            caption = i[1]\n            train_input=i[2]\n            test_input=i[3]\n            val_input=i[4]            \n            if img_name in images_features:\n                if train_input=='TRUE':\n                    captions_dict_train[img_name] = caption \n                elif test_input=='TRUE':\n                    captions_dict_test[img_name] = caption\n                elif val_input=='TRUE':\n                    captions_dict_val[img_name] = caption\n        except:        \n            pass\n        \nprint(\"size of train, test and validation dataset:\" ,len(captions_dict_train),len(captions_dict_test),len(captions_dict_val))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T05:00:59.196780Z","iopub.execute_input":"2023-12-08T05:00:59.197709Z","iopub.status.idle":"2023-12-08T05:00:59.457322Z","shell.execute_reply.started":"2023-12-08T05:00:59.197663Z","shell.execute_reply":"2023-12-08T05:00:59.456515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessed(txt):\n    modified = txt.lower().replace('ред',' ред').replace(\"'\",\"\")\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified\n\ncount_words = {}\n# print(\"Captions before preprocessing\\n \",list(captions_dict_train.items())[:5])\n\nfor k,v in captions_dict_train.items():\n    captions_dict_train[k]=preprocessed(v)\n    \n    \n#creating count_words dict with word and frequency\nfor k,v in captions_dict_train.items():\n    for word in v.split():\n        if word not in count_words:\n            count_words[word] = 1\n        else:\n            count_words[word] += 1\n\n#creating words_dict as vocabulary where only words occuring more than threshold is considered\nTHRESH = 5\ncount = 1\nwords_dict = {}\nfor k,v in count_words.items():\n    if count_words[k] > THRESH:\n        words_dict[k] = count\n        count += 1\n\n#captions_dict converting words into numbers from words_dict       \nfor k, v in captions_dict_train.items():    \n    encoded = []\n    for word in v.split():\n        if word in words_dict.keys():\n            encoded.append(words_dict[word])\n            \n    captions_dict_train[k] = encoded\n    \nprint(\"training Captions after preprocessing\\n \",list(captions_dict_train.items())[:5])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T05:01:02.826814Z","iopub.execute_input":"2023-12-08T05:01:02.827537Z","iopub.status.idle":"2023-12-08T05:01:03.662567Z","shell.execute_reply.started":"2023-12-08T05:01:02.827497Z","shell.execute_reply":"2023-12-08T05:01:03.661824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a similar validation captions dictionary captions_dict_val\n\n# Preprocess validation captions\nfor k, v in captions_dict_val.items():\n    captions_dict_val[k] = preprocessed(v)\n\n# Create count_words dict with word and frequency for validation set\nfor k, v in captions_dict_val.items():\n    for word in v.split():\n        if word not in count_words:\n            count_words[word] = 1\n        else:\n            count_words[word] += 1\n\n# Create words_dict as vocabulary for validation set\nwords_dict_val = {}\ncount_val = 1\nfor k, v in count_words.items():\n    if count_words[k] > THRESH:\n        words_dict_val[k] = count_val\n        count_val += 1\n\n# Convert validation captions to numbers using words_dict_val\nfor k, v in captions_dict_val.items():\n    encoded_val = []\n    for word in v.split():\n        if word in words_dict_val.keys():\n            encoded_val.append(words_dict_val[word])\n\n    captions_dict_val[k] = encoded_val\n\nprint(\"Validation Captions after preprocessing\\n\", list(captions_dict_val.items())[:5])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T05:01:06.535337Z","iopub.execute_input":"2023-12-08T05:01:06.536092Z","iopub.status.idle":"2023-12-08T05:01:06.703835Z","shell.execute_reply.started":"2023-12-08T05:01:06.536033Z","shell.execute_reply":"2023-12-08T05:01:06.703085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"./words_dict_nepali_sc.pkl\", \"wb\") as f:\n   pickle.dump(words_dict , f)   \n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T04:13:55.545681Z","iopub.execute_input":"2023-12-08T04:13:55.546404Z","iopub.status.idle":"2023-12-08T04:13:55.553719Z","shell.execute_reply.started":"2023-12-08T04:13:55.546365Z","shell.execute_reply":"2023-12-08T04:13:55.553113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge Model","metadata":{}},{"cell_type":"code","source":"\nvocab_size = len(words_dict)+1\nMAX_LEN = 0\n\nfor k, v in captions_dict_train.items():\n    if len(v) > MAX_LEN:\n        MAX_LEN = len(v)  \n        \n# feature extractor model\ninputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\n# language sequence model\ninputs2 = Input(shape=(MAX_LEN,))\nse1 = Embedding(vocab_size, MAX_LEN, mask_zero=True)(inputs2)\nse2 = Dropout(0.4)(se1)\nse3 = LSTM(256)(se2)\n\n# decoder model\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n# tie it together [image, seq] [word]\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n# summarize model\n# model.summary()\n# plot_model(model, show_shapes=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T05:02:09.301971Z","iopub.execute_input":"2023-12-08T05:02:09.302681Z","iopub.status.idle":"2023-12-08T05:02:10.120153Z","shell.execute_reply.started":"2023-12-08T05:02:09.302646Z","shell.execute_reply":"2023-12-08T05:02:10.119523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training RNN- Using Generator Function to create input","metadata":{}},{"cell_type":"code","source":"N=32 #BatchSize\nVOCAB_SIZE = len(words_dict)+1\n\ndef progressive_generator(photo_dict, caption_dict, MAX_LEN,VOCAB_SIZE):\n    while 1:\n        for i in range(0,len(caption_dict),N):\n            caption = dict(list(caption_dict.items())[0+i: N+i])\n            X, y_in, y_out = create_sequences(photo_dict,caption,MAX_LEN, VOCAB_SIZE)\n            yield [X, y_in], y_out       \n    \n\ndef create_sequences(photo, caption, MAX_LEN,VOCAB_SIZE):\n    #n_samples = 0    \n    X,y_in,y_out = [],[],[]\n    \n    for k, v in caption.items():   \n        for i in range(1, len(v)):\n            X.append(photo[k])\n    \n            in_seq= [v[:i]]\n            out_seq = v[i]\n    \n            in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n            out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n    \n            y_in.append(in_seq)\n            y_out.append(out_seq)\n            \n    return np.array(X), np.array(y_in), np.array(y_out)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T04:14:11.796436Z","iopub.execute_input":"2023-12-08T04:14:11.797069Z","iopub.status.idle":"2023-12-08T04:14:11.807131Z","shell.execute_reply.started":"2023-12-08T04:14:11.797019Z","shell.execute_reply":"2023-12-08T04:14:11.806323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# steps = len(captions_dict_train)/N\n# generator = progressive_generator(images_features, captions_dict_train, MAX_LEN, VOCAB_SIZE)\n# model.fit(generator, epochs=50, steps_per_epoch=steps, verbose=1)\n# model.save('./cultural_nepali_50_test' + '.h5')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:14:23.479780Z","iopub.execute_input":"2023-12-07T16:14:23.480377Z","iopub.status.idle":"2023-12-07T16:14:23.495922Z","shell.execute_reply.started":"2023-12-07T16:14:23.480336Z","shell.execute_reply":"2023-12-07T16:14:23.494845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# model.save('./image_caption_model_newarch_stanford_nepali50' + '.h5')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T01:50:07.433716Z","iopub.execute_input":"2023-12-07T01:50:07.434494Z","iopub.status.idle":"2023-12-07T01:50:07.520579Z","shell.execute_reply.started":"2023-12-07T01:50:07.434453Z","shell.execute_reply":"2023-12-07T01:50:07.519779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:58:23.264919Z","iopub.execute_input":"2022-04-22T17:58:23.265229Z","iopub.status.idle":"2022-04-22T17:58:23.275284Z","shell.execute_reply.started":"2022-04-22T17:58:23.265196Z","shell.execute_reply":"2022-04-22T17:58:23.274612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Prediction","metadata":{}},{"cell_type":"code","source":"vocab_size = len(words_dict)+1\nMAX_LEN = 0\n\nfor k, v in captions_dict_train.items():\n    if len(v) > MAX_LEN:\n        MAX_LEN = len(v)  \n        \ninv_dict = {v:k for k, v in words_dict.items()}","metadata":{"execution":{"iopub.status.busy":"2023-12-07T08:24:04.435848Z","iopub.execute_input":"2023-12-07T08:24:04.436594Z","iopub.status.idle":"2023-12-07T08:24:04.447587Z","shell.execute_reply.started":"2023-12-07T08:24:04.436553Z","shell.execute_reply":"2023-12-07T08:24:04.446704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For selected test images\n\n#2373586.jpg 75 epoch\n#2361833.jpg 75 epoch\n\n#2394335.jpg 150 epoch\n#2364210.jpg 150 epoch\n#2402430.jpg 2395361.jpg 150 epoch\n#2335374.jpg 2373374.jpg 150 epoch\n\n\nmodel = tf.keras.models.load_model('../input/imagecaptioningmodels/image_caption_model_newarch150.h5')\n# img_name='01659403106.jpg'\n# test_feature = images_features[img_name]\ntest_img_path = '../input/paragraph/01659403106.jpg'\ntest_img = cv2.imread(test_img_path)\ntest_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\ntext_inp = ['startofseq']\ncount = 0\ncaption = ''\nwhile count < MAX_LEN:\n    count += 1\n    encoded = []\n    for i in text_inp:\n        encoded.append(words_dict[i])\n    encoded = [encoded]\n    encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n    data_list=[np.array(test_feature).reshape(1,-1), np.array(encoded).reshape(1,-1)]        \n    prediction = np.argmax(model.predict(data_list))        \n    sampled_word = inv_dict[prediction]\n    caption = caption + ' ' + sampled_word\n        \n    if sampled_word == 'endofseq':\n        break\n    text_inp.append(sampled_word)\n    \ncaption= caption.replace('endofseq','') \npredicted= caption.split()\n# actual= captions_dict_test[img_name].split()\n\n# blueScore= sentence_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n  \n# print('BLEU-1: %f' % sentence_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n# print('BLEU-2: %f' % sentence_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n# print('BLEU-3: %f' % sentence_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n# print('BLEU-4: %f' % sentence_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\nplt.figure()\nplt.imshow(test_img)    \n# plt.title(img_name+\" - BLUE Score: \"+str(blueScore))\nplt.show()\nprint(caption.replace(' .','.'))","metadata":{"execution":{"iopub.status.busy":"2023-07-06T11:41:19.213793Z","iopub.execute_input":"2023-07-06T11:41:19.214188Z","iopub.status.idle":"2023-07-06T11:41:33.353706Z","shell.execute_reply.started":"2023-07-06T11:41:19.21415Z","shell.execute_reply":"2023-07-06T11:41:33.352826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For selected test images\n\n#2373586.jpg 75 epoch\n#2361833.jpg 75 epoch\n\n#2394335.jpg 150 epoch\n#2364210.jpg 150 epoch\n#2402430.jpg 2395361.jpg 150 epoch\n#2335374.jpg 2373374.jpg 150 epoch\n#ktm_154\n#p_168\n\n\nmodel = tf.keras.models.load_model('/kaggle/input/modal-file/cultural_nepali_50_test.h5')\nimg_name='ktm_154.jpg'\ntest_feature = images_features[img_name]\ntest_img_path = '/kaggle/input/stanford-cultural-images/stanford_img/content/stanford_images/'+img_name\ntest_img = cv2.imread(test_img_path)\ntest_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\ntext_inp = ['startofseq']\ncount = 0\ncaption = ''\nwhile count < MAX_LEN:\n    count += 1\n    encoded = []\n    for i in text_inp:\n        encoded.append(words_dict[i])\n    encoded = [encoded]\n    encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n    data_list=[np.array(test_feature).reshape(1,-1), np.array(encoded).reshape(1,-1)]        \n    prediction = np.argmax(model.predict(data_list))        \n    sampled_word = inv_dict[prediction]\n    caption = caption + ' ' + sampled_word\n        \n    if sampled_word == 'endofseq':\n        break\n    text_inp.append(sampled_word)\n    \ncaption= caption.replace('endofseq','') \npredicted= caption.split()\nactual= captions_dict_test[img_name].split()\n\nblueScore= sentence_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n  \nprint('BLEU-1: %f' % sentence_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint('BLEU-2: %f' % sentence_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\nprint('BLEU-3: %f' % sentence_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\nprint('BLEU-4: %f' % sentence_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\nplt.figure()\nplt.imshow(test_img)    \nplt.title(img_name+\" - BLUE Score: \"+str(blueScore))\nplt.show()\nprint(caption.replace(' .','.'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For random test images\n\nmodel = tf.keras.models.load_model('../input/imagecaptioningmodels/image_caption_model_newarch75.h5')\n\nplt.figure()\nfor i in range(3):\n    \n    num=  np.random.randint(0,len(captions_dict_test))    \n    img_name=list(captions_dict_test)[num]\n    test_feature = images_features[img_name]\n    test_img_path = '/kaggle/input/testpucture/'+img_name\n\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n    \n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < MAX_LEN:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(words_dict[i])\n\n        encoded = [encoded]\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n        data_list=[np.array(test_feature).reshape(1,-1), np.array(encoded).reshape(1,-1)]        \n        prediction = np.argmax(model.predict(data_list))        \n        sampled_word = inv_dict[prediction]\n        caption = caption + ' ' + sampled_word\n            \n        if sampled_word == 'endofseq':\n            break\n\n        text_inp.append(sampled_word)\n        \n    caption= caption.replace('endofseq','')\n    predicted= caption.split()\n    actual= captions_dict_test[img_name].split()\n    \n    blueScore= sentence_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n\n    plt.imshow(test_img)    \n    plt.title(img_name+\" - BLUE Score: \"+str(blueScore))\n    plt.show()\n    print(caption.replace(' .','.') )\n    print('BLEU-1: %f' % sentence_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % sentence_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % sentence_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % sentence_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n    \n                               ","metadata":{},"execution_count":null,"outputs":[]}]}